{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning\n",
    "\n",
    "Jay Urbain, PhD\n",
    "\n",
    "3/22/2017\n",
    "\n",
    "References:\n",
    "\n",
    "- Games, G., Witten, D., Hastie, T., and Tibshirani, R. (2013) An Introduction to Statistical Learning\n",
    "with applications in R, www.StatLearning.com, Springer-Verlag, New York. Chapter 4.\n",
    "\n",
    "- [scikit-learn](http://scikit-learn.org/stable/)\n",
    "\n",
    "In this notebook we will be applying logistic regression classification using *scikit-learn* to classify instances as likely or not likely to default using the *Credit Card Default* dataset.\n",
    "\n",
    "Answer **questions** in the empty cell(s) below each question in the notebook. Here are the steps we will follow. Also summarize your answers to each question in your lab report.\n",
    "\n",
    "1 - Importing the data\n",
    "\n",
    "2 - The Logistic Function\n",
    "\n",
    "3 - Getting, preparing, and visualizing the data\n",
    "\n",
    "4 - Logistic Regression classification\n",
    "\n",
    "5 - Evaluating the model on the training data\n",
    "\n",
    "6 - Summary for applying Logistic Regression with scikit-learn\n",
    "\n",
    "7 - Questions\n",
    "\n",
    "Dataset: **Credit Card Default Data**\n",
    "\n",
    "Description: A simulated data set containing information on ten thousand customers. The aim here is to predict\n",
    "which customers will default on their credit card debt. A data frame with 10000 observations on the following 4 variables:\n",
    "\n",
    "- **default** A factor (categorical attribute) with levels (enumerated values) No and Yes indicating whether the customer defaulted on their debt.\n",
    "\n",
    "- **student** A factor with levels No and Yes indicating whether the customer is a student.\n",
    "\n",
    "- **balance** The average balance that the customer has remaining on their credit card after making\n",
    "their monthly payment.\n",
    "\n",
    "- **income** Income of customer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the following libraries if you have not already done so. \n",
    "\n",
    "[Searborn statistical](https://stanford.edu/~mwaskom/software/seaborn/) visualization library installation\n",
    "\n",
    "Use the conda package installing in a terminal window as follows:\n",
    "\n",
    "conda install seaborn\n",
    "    \n",
    "[Statsmodel](http://statsmodels.sourceforge.net/) allows you to explore data, estimate statistical models, and perform statistical tests.\n",
    "\n",
    "Use the conda package installing in a terminal window as follows:\n",
    "\n",
    "conda install statsmodels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Dataset Import\n",
    "import statsmodels.api as sm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "# plots within notebook versus launching a separate window\n",
    "%matplotlib inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. The Logistic Function\n",
    "\n",
    "The [Logistic Function](http://en.wikipedia.org/wiki/Logistic_function) takes an input from negative to positive infinity and outputs a value between 0 and 1. \n",
    "\n",
    "The logistic function is defined as:\n",
    "\n",
    "$ g(z)= \\frac{1}{1+e^{-z}}$\n",
    "\n",
    "Where $z=\\beta^TX = \\beta _0 x_{i_0} + ... + \\beta_p x_{ip}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the [Logisitc Function](http://matplotlib.org/users/mathtext.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# generate 1000 samples between -20 and 20\n",
    "x = np.linspace(-20, 20, 1000)\n",
    "beta = [0, 0.5]\n",
    "y = np.exp(beta[0] + beta[1]*x) / (1 + np.exp(beta[0] + beta[1]*x))\n",
    "\n",
    "# Plotting\n",
    "plt.plot(x, y, 'r', alpha=0.75, linewidth=2.5)\n",
    "plt.plot([0,0], [0, 1], 'k')\n",
    "plt.plot([-20,20], [0.5, 0.5], 'k')\n",
    "plt.xlabel(r'$z$', fontsize='xx-large')\n",
    "plt.ylabel(r'$g(z)$', fontsize='xx-large')\n",
    "\n",
    "print ('Figure 1. Logistic Function')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can interpret $g(z)$  as the probability of positive classification. Values $>= 0.5$ are classified as positive (1: likely to default), and values $< 0.5$ are classified as negative (0: not likely to default)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Getting, preparing, and visualizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read Default.csv into a DataFrame\n",
    "d = pd.read_csv('../data/Default.csv')\n",
    "d.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print (d.describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the data for classification\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert classification attribute to numeric binary value for binary classification\n",
    "\n",
    "d.loc[d.student == 'Yes', 'student'] = 1\n",
    "d.loc[d.student == 'No', 'student'] = 0\n",
    "print( d.head() )\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X = d[['balance','student','income']]\n",
    "y = d.default\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "\n",
    "# Convert the split data back into a DataFrame\n",
    "train = pd.DataFrame(data=X_train, columns=['balance','student','income'])\n",
    "train['default'] = y_train\n",
    "test = pd.DataFrame(data=X_test, columns=['balance','student','income'])\n",
    "test['default'] = y_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a histogram of all variables\n",
    "# Histograms show the distribution of each predictor variable\n",
    "train.hist()\n",
    "print('Figure 2. Histogram of all variables')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a scatter plot of the income vs. balance\n",
    "# Scatterplots are useful for visualizing the association between two variables\n",
    "train.plot(x='balance', y='income', kind='scatter', alpha=0.3)\n",
    "print ('Figure 3. Scatterplot of incomve vs. balance')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We can mark defaults (cassification attribute) with a different color and symbol\n",
    "train_nd = d[d.default == 0]\n",
    "train_d = d[d.default == 1]\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(train_nd.balance, train_nd.income, alpha = .5, marker='+', c= 'b')\n",
    "\n",
    "plt.scatter(train_d.balance, train_d.income, marker='o', \n",
    "            edgecolors = 'r', facecolors = 'none')\n",
    "plt.legend( ('default', 'no default'), loc='upper right')\n",
    "plt.xlabel(\"Balance $\")\n",
    "plt.ylabel(\"Income $\")\n",
    "plt.title ('Figure 4. Scatterplot of incomve vs. balance - default, non-default')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1) What precentage of individuals in the dataset default?** Answer in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2) From 'Figure 4. Scatterplot of income vs. balance - default, non-default':**\n",
    "\n",
    "Can you identfy an income threshold for default vs. non-default? If so, what is it?\n",
    "\n",
    "Can you identfy a balance threshold for default vs. non-default? If so, what is it?\n",
    "\n",
    "Which individual attribute, i.e., income or balance is likely to be the more accurate predictor? \n",
    "\n",
    "Provide your answers in the cell below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Logistic Regression classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fit (learn) a logistic regression model on the balance variable\n",
    "balance = LogisticRegression()\n",
    "balance.fit(train[['balance']], y_train)\n",
    "\n",
    "# Is the beta_1 value associated with balance significant?\n",
    "B1 = balance.coef_[0][0]\n",
    "B0 = balance.intercept_[0]\n",
    "print ('B1: ', B1, ' B0: ', B0)\n",
    "print ('e^B1: ', np.exp(B1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\beta_1$ is significant.\n",
    "\n",
    "Logistic function: $\\dfrac{g(z)}{1-g(z)} = \\dfrac{e^{\\beta_0+\\beta_1x}}{1-e^{\\beta_0+\\beta_1x}}$\n",
    "\n",
    "The log-odds can be defined as the log odds of positive classification:\n",
    "\n",
    "log odds = $log\\big(\\dfrac{e^{\\beta_0+\\beta_1x}}{1-e^{\\beta_0+\\beta_1x}}\\big) = \\beta_0+\\beta_1x$\n",
    "\n",
    "Increasing $X$ by one unit changes the log odds by $\\beta_1$, or equivalently \n",
    "it multiplies the odds by $e^{\\beta_1}$. \n",
    "\n",
    "Because the relationship between $f(X)$ and $X$ is not a straight line, $\\beta_1$ does not correspond to the change \n",
    "in $f(X)$ associated with a one-unit increase in $X$. The amount that \n",
    "$f(X)$ changes due to a one-unit change in $X$ will depend on the current \n",
    "value of $X$.\n",
    "\n",
    "In our example, ${\\beta_1} ~= 0.0043$\n",
    "\n",
    "Log-odds: \n",
    "If you increase $x$ by 1, you increase the log-odds by $0.0043*1$. \n",
    "If you increase $x$ by 800, you increase the log-odds by 0.0043*800 = 3.44\n",
    "\n",
    "To get the change in $f(X)$, if you increase $x$ by 1, you multiply the odds by $e^{0.0043}$. \n",
    "If you increase $x$ by 800, you mutliply the odds by $e^{0.0043*800} = 31.187$, not $800 * e^{0.0043}$\n",
    "\n",
    "[Log-odds](https://en.wikipedia.org/wiki/Logit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction using the *balance* variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Predict the probability of default for someone with a balance of $1.2k and $2.5k\n",
    "prob = balance.predict(pd.DataFrame({'balance': [1200, 2500]}))\n",
    "\n",
    "# What does beta mean? Every unit increase in balance corresponds to a beta increase in y. \n",
    "x = np.linspace(test.balance.min(), test.balance.max(),500)\n",
    "beta = [B0,B1]\n",
    "\n",
    "y = np.exp(beta[0] + beta[1]*x) / (1 + np.exp(beta[0] + beta[1]*x))\n",
    "odds = np.exp(beta[0] + beta[1]*x)\n",
    "log_odds = beta[0] + beta[1]*x\n",
    "\n",
    "plt.figure(figsize=(7, 8))\n",
    "plt.subplot(311)\n",
    "plt.plot(x, y, 'r', linewidth=2)\n",
    "plt.ylabel('Probability')\n",
    "plt.text(500, 0.7, r'$\\frac{e^{\\beta_o + \\beta_1x}}{1+e^{\\beta_o + \\beta_1x}}$', fontsize=25)\n",
    "\n",
    "plt.subplot(312)\n",
    "plt.plot(x, odds, 'k', linewidth=2)\n",
    "plt.ylabel('Odds')\n",
    "plt.text(500, 10, r'$e^{\\beta_o + \\beta_1x}$', fontsize=20)\n",
    "\n",
    "plt.subplot(313)\n",
    "plt.plot(x, log_odds, 'c', linewidth=2)\n",
    "plt.ylabel('Log(Odds)')\n",
    "plt.xlabel('x')\n",
    "plt.text(500, 1, r'$\\beta_o + \\beta_1x$', fontsize=15)\n",
    "print ('Figure 5. Probability, Odds, and Log(odds) of balance - default, non-default')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot the fitted logistic function overtop of the data points\n",
    "plt.figure()\n",
    "plt.scatter(test.balance, test.default, alpha=0.1)\n",
    "plt.plot(x, y, 'r', linewidth=2)\n",
    "plt.xlabel(\"Balance\")\n",
    "plt.ylabel(\"Probability of Default\")\n",
    "plt.ylim([-0.05,1.05]); plt.xlim([0, 2800])\n",
    "plt.plot([1200, 2500], prob, 'ko')\n",
    "print ('Figure 6. The fitted logistic model for predicting default from balance')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Evaluating the model on the training data\n",
    "\n",
    "Accuracy measures the overall correctness of classification. \n",
    "\n",
    "Accuracy = $\\dfrac{TP+TN}{TP+TN+FP+FN}$\n",
    "\n",
    "Sensitivity (also called the true positive rate, or recall) measures the proportion of positives that are correctly identified as such. E.g., people who have cancer.\n",
    "\n",
    "Sensitivity = $\\dfrac{TP}{TP+FN}$\n",
    "\n",
    "Specificity (also called the true negative rate) measures the proportion of negatives that are correctly identified as such. E.g., people who do not have cancer.\n",
    "\n",
    "Specificity = $\\dfrac{TN}{TN+FP}$\n",
    "\n",
    "[Sensititivy and specificity](https://en.wikipedia.org/wiki/Sensitivity_and_specificity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create predictions using the balance model on the test set\n",
    "test['pred_class'] = balance.predict(test[['balance']])\n",
    "\n",
    "# Accuracy\n",
    "accuracy = sum(test.pred_class == test.default) / float(len(test.default))\n",
    "print ('Accuracy: ', accuracy)\n",
    "\n",
    "# Sensitivity\n",
    "# For those who did default, how many did it predict correctly? \n",
    "test_d = test[test.default == 1]\n",
    "sensitivity = sum(test_d.pred_class == test_d.default) / float(len(test_d.default))\n",
    "print ('Sensitivity: ', sensitivity)\n",
    "\n",
    "# Specificity\n",
    "# For those who didn't default, how many did it predict correctly?\n",
    "test_nd = test[test.default == 0]\n",
    "specificity = sum(test_nd.pred_class == test_nd.default) / float(len(test_nd.default))\n",
    "print ('Specificity: ', specificity)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class imbalance can result in accuracy measures that are missleading. How does our overall classification accuracy compare to the non-default rate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "null = 1 - sum(d.default) / float(len(d.default))\n",
    "print ('Classification accuracy vs. non-default rate: ', null)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What this means is that you could have just guessed not going to default, and would be correct 96.67% of the time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Multivariate Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fit (learn) a logistic regression model on the balance variable\n",
    "balance_student = LogisticRegression()\n",
    "balance_student.fit(train[['balance','student']], y_train)\n",
    "\n",
    "B2 = balance_student.coef_[0][1]\n",
    "B1 = balance_student.coef_[0][0]\n",
    "B0 = balance_student.intercept_[0]\n",
    "print ('B0: ', B0, 'B1: ', B1, 'B2: ', B2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3) For a given income level, who are more likely to default, students or non-students?**\n",
    "\n",
    "*Hint: look at the value of the coefficients for the balance_student model created in Step 6. $B0$ is the intercept, $B1$ is the coefficent for income, and $B2$ is the coefficient for student (1=student). Think about the meaning of the coefficients.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q4) Which combination of predictors: student, balance, or income has the highest predictive accuracy?**\n",
    "\n",
    "To answer this question, we will use a feature selection technique called forward selection.\n",
    "\n",
    "- Build 3 separate models for each predictor using the 70/30 split training/test set created above. See *step 6, Multivariate Logisitc Regression* for an example. Of the three, select the most predicitve univariate model. For example, income.\n",
    "\n",
    "- Next build 2 models: income+balance, and income+student. If neither of these models perform better than the previous income model, stop.\n",
    "\n",
    "- Else build 1 model with all three predictors. \n",
    "\n",
    "Show your work, and summarize your results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Summary for applying Logistic Regression with scikit-learn\n",
    "\n",
    "Create LogisticRegression model:\n",
    "\n",
    "*model = LogisticRegression()*\n",
    "\n",
    "Fit a model to our data:\n",
    "\n",
    "*model.fit(X,Y)*\n",
    "\n",
    "Check our accuracy:\n",
    "\n",
    "*model.score(X,Y)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
